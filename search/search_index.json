{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"GeoEPIC","text":""},{"location":"#a-toolkit-for-geospatial-crop-simulations","title":"A toolkit for geospatial Crop Simulations","text":"<p>This package expands the capabilities of the EPIC crop simulation model, to simulate crop growth and development across large geographies, such as entire states or counties by leveraging openly availabe remote sensing products and geospatial databases. Additionally, the toolkit features a unique calibration module that allows fine-tuning of model parameters to reflect specific local conditions or experimental results. This toolkit allows researchers to assess crop production potential, management scenarios and risks at broader scales, informing decision-making for sustainable agricultural practices.</p>"},{"location":"#installation","title":"Installation","text":"<p>Before starting the setup, ensure you have <code>wget</code> and <code>conda</code> installed. Follow the links for corresponding installation guides. Find the detailed instructions in the setup section.</p> <p><pre><code>conda create --name epic_env python=3.11.9\n</code></pre> <pre><code>conda activate epic_env\n</code></pre> <pre><code>pip install git+https://github.com/smarsGroup/geo-epic.git\n</code></pre></p>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Crop Yield Forecasting: Used in studies predicting crop yields under different climate scenarios.</li> <li>Sustainable Agriculture Tools: Incorporated into platforms promoting sustainable farming practices.</li> </ul>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Bharath Irigireddy, Varaprasad Bandaru, Sachin Velmurgan, Rohit Nandan, SMaRS Group</li> </ul>"},{"location":"OPC/","title":"Crop Management","text":"<p>OPC input files refers to the agricultural management practice files. Creating these files is not yet implemented in the toolkit. Refer to the epic user manual on how to prepare the management files.</p>"},{"location":"Soil/","title":"Soil Module","text":"<p>In agriculture simulations, such as those conducted using the EPIC model, soil data directly influences water availability, nutrient supply, and overall crop growth predictions. Soil input files to the model contain detailed information about the soil properties of a specific location, depth wise. GeoEPIC helps in generating soil files required by the EPIC Model from two soil data sources: </p> <ul> <li>USDA SSURGO: which contains detailed surveys of U.S. soils. </li> <li>ISRIC SoilGrids 250m: which offers global coverage in a grid format.</li> </ul> <p></p>"},{"location":"Soil/#usage","title":"Usage","text":""},{"location":"Soil/#usda-ssurgo","title":"USDA SSURGO","text":"<p>The USDA Soil Survey Geographic (SSURGO) database is a comprehensive resource for soil data collected by the Natural Resources Conservation Service (NRCS) across the United States and the Territories. This database provides detailed information on soil properties and classifications. The data is collected through extensive field surveys and laboratory analysis. For more detailed information, visit the USDA NRCS SSURGO page.</p> <p>To fetch and output soil files using the USDA SSURGO database, following commands could be used. For a specific location, specify the latitude and longitude coordinates to generate a soil file named {mukey}.SOL. </p> <p><pre><code># Fetch and output soil files for a specific latitude and longitude\n&gt;&gt; geo_epic soil usda --fetch {lat} {lon} --out {out_path}\n</code></pre> <pre><code># Fetch for a list of locations in a csv file with lat, lon\n&gt;&gt; geo_epic soil usda --fetch {list.csv} --out {out_dir}\n</code></pre> <pre><code># Fetch for crop sequence boundaries shape file.\n&gt;&gt; geo_epic soil usda --fetch {aoi_csb.shp} --out {out_dir}\n</code></pre></p> <p>Note: This command will write Soil IDs (mukeys) corresponding to each location as an attribute into the input file, when used with a CSV file or crop sequence boundary shapefile.</p> <p>Processing ssurgo gdb file:</p> <p>To process a SSURGO GDB file and generate soil files for all unique soils contained in it, follow these steps. For instance, if you require soil files for Maryland, navigate to the 'State Database - Soils' section, and download the 'gSSURGO_MD.zip' file. Once the download is complete, extract the contents and place the GDB file in the 'soil' folder within your workspace. Use the following command to generate the soil files for all mukeys. </p> <p>Link: https://www.nrcs.usda.gov/resources/data-and-reports/gridded-soil-survey-geographic-gssurgo-database</p> <pre><code>&gt;&gt; geo_epic soil process_gdb -i {path/to/ssurgo.gdb} -o {out_dir} \n</code></pre>"},{"location":"Soil/#isric-soil-grids-data","title":"ISRIC Soil Grids Data","text":"<p>The ISRIC Soil Grids 250 meters database is an advanced resource providing high-resolution global soil information. Managed by the International Soil Reference and Information Centre (ISRIC), it uses state-of-the-art machine learning methods to map soil properties across the globe. The prediction models are based on soil profile observations and environmental covariates, including climate, land cover, and terrain morphology. SoilGrids offers soil properties at six standard depth intervals at 250 meters spatial resolution. For more detailed information, visit the ISRIC Soil Grids page.</p> <p></p> <p>To fetch and output soil files using the ISRIC Soil Grids 250 meters database, you can use the following commands. <pre><code># Fetch and output soil files for a specific latitude and longitude\n&gt;&gt; geo_epic soil soilgrids --fetch {lat} {lon} --out {out_path}\n</code></pre> <pre><code># Fetch for a list of locations in a csv file with lat, lon, out_path columns\n&gt;&gt; geo_epic soil soilgrids --fetch {list.csv} --out {column_name}\n</code></pre></p> <p>Once the tool finishes running, you should see files named {out_name}.SOL in your specified output directory. Open this file to verify that it contains the necessary soil data formatted correctly for use in the EPIC model.  Refer to the EPIC manual for Soil file formatting</p>"},{"location":"Weather/","title":"Weather Module","text":"<p>Weather data is vital in providing essential environmental inputs that significantly influence crop growth and development. Reliable weather inputs ensure that agricultural simulations reflect realistic responses to climatic conditions. The EPIC model requires weather input files that detail daily and monthly climatic variables. Daily files provide day-to-day weather data, while monthly files summarize the average or total values per month. These files are crucial for driving the daily simulation processes in EPIC.</p>"},{"location":"Weather/#fetching-weather-data","title":"Fetching Weather Data","text":"<p>GeoEPIC streamlines the process of fetching and organizing weather data essential for EPIC simulations. It supports the creation of weather input files from daymet, and also from sophisticated datasets available through Google Earth Engine. One can define a composite collection in the config file, detailing the specific variables to select from each data source, tailoring the dataset to meet the needs of their simulations. </p>"},{"location":"Weather/#using-gee-collections","title":"Using GEE collections","text":"<p>GeoEPIC allows the integration of various weather and climate data sources on GEE. To explore the available datasets, visit Google Earth Engine's dataset catalog and GEE Community Catalog. Private assets can also be uploaded to Earth Engine, to use them in combination with existing datasets. Below is an example of configuration file that can be used to create weather input files.</p> <p>Example config files:</p> <ul> <li>using AgERA5</li> </ul> <pre><code># Global parameters\nglobal_scope:\n  time_range: ['2002-01-01', '2022-12-31']\n  variables: ['srad', 'tmax', 'tmin', 'prcp', 'rh', 'ws']  \n  resolution: 9600\n\n\n# Specify Earth Engine (EE) collections and their respective variables\ncollections:\n  AgEra5:\n    collection: 'projects/climate-engine-pro/assets/ce-ag-era5/daily'\n    variables:\n      srad: b('Solar_Radiation_Flux') \n      tmax: b('Temperature_Air_2m_Max_24h') - 273.15\n      tmin: b('Temperature_Air_2m_Min_24h') - 273.15\n      prcp: b('Precipitation_Flux') \n      rh: b('Relative_Humidity_2m_06h')\n      ws: b('Wind_Speed_10m_Mean')\n</code></pre> <p>Using the config file to get data:</p> <p><pre><code># Fetch and output weather input files for a specific latitude and longitude\n&gt;&gt; geo_epic weather config.yml --fetch {lat} {lon} --out {out_path}\n</code></pre> <pre><code># Fetch for a list of locations in a csv file with lat, lon, out_path columns\n&gt;&gt; geo_epic weather config.yml --fetch {list.csv} --out {column_name}\n</code></pre> <pre><code># Fetch for crop sequence boundaries shape file.\n&gt;&gt; geo_epic weather config.yml --fetch {aoi_csb.shp} --out {out_dir}\n</code></pre></p> <p>Note: This command will write weather grid IDs corresponding to each location as an attribute into the input file, when used with a CSV file or crop sequence boundary shapefile.</p>"},{"location":"Weather/#from-daymet-and-nldas","title":"From Daymet and NLDAS","text":"<p>To fetch weather data for a specific latitude and longitude using the Daymet dataset, start by downloading the windspeed data from the NLDAS dataset and saving it to the specified directory. Afterward, fetch the other required weather data from the Daymet dataset.</p> <p><pre><code># To download windspeed data, use the following command\n&gt;&gt; geo_epic weather windspeed -bbox &lt;bounding_box&gt; -start &lt;start_date&gt; -end &lt;end_date&gt; -out './weather'\n</code></pre> <pre><code># To fetch and output weather input files for a specific latitude and longitude\n&gt;&gt; geo_epic weather daymet --fetch {lat} {lon} -start &lt;start_date&gt; -end &lt;end_date&gt; --out './weather'\n</code></pre></p>"},{"location":"api/","title":"API","text":""},{"location":"api/#commands-available","title":"Commands Available","text":""},{"location":"api/#todo","title":"(ToDo)","text":"<p>GeoEPIC allows you to run various commands. The structure is as show below:</p> <p><pre><code>geo_epic {module} {func} -options\n</code></pre> example usage: <pre><code>geo_epic workspace new -w Test\n</code></pre></p>"},{"location":"api/#list-of-modules-and-functions","title":"List of Modules and Functions:","text":""},{"location":"api/#workspace","title":"workspace","text":"<ul> <li>new: Create a new workspace with a template structure.</li> <li>prepare: Prepare the input files using config file.</li> <li>run: Execute the simulations.</li> <li>post_process: Process Output files from simulation runs.</li> </ul>"},{"location":"api/#weather","title":"weather","text":"<ul> <li>ee: </li> <li>windspeed: </li> <li>download_daily: Download daily weather data. </li> <li>daily2monthly: Convert daily weather data to monthly.</li> </ul>"},{"location":"api/#soil","title":"soil","text":"<ul> <li>usda:</li> <li>isirc:</li> <li>process_gdb: Process ssurgo gdb file.</li> </ul>"},{"location":"api/#sites","title":"sites","text":"<ul> <li>process_foi: Process fields of interest file.  (TODO)</li> <li>generate: Generate site files from processed data.</li> </ul> <p>For more details on each command and its options, use: <pre><code>geo_epic {module} {func} --help\n</code></pre></p>"},{"location":"csb/","title":"Homogeneous Land Units","text":"<p>For effective simulations using the EPIC model, it is essential to have land units that are characterized by uniform soil properties and consistent management practices. One way to achieve this for the USA is by utilizing the Crop Sequence Boundaries (CSB) dataset.</p>"},{"location":"csb/#crop-sequence-boundaries","title":"Crop Sequence Boundaries","text":"<p>The CSB dataset is derived by analyzing annual variations in the USDA's Cropland Data Layer (CDL). The CSB method involves identifying and demarcating the boundaries of agricultural fields that have maintained consistent crop rotation sequence over several years. </p> <p></p> <p>Crop Sequence Boundaries (CSB): Delineated Fields Using Remotely Sensed Crop Rotations.  USDA-NASS &amp; Global Conservation Institute. Hunt, K. A., Abernethy, J., Beeson, P., Bowman, M., Wallander, S., &amp; Williams, R.</p>"},{"location":"csb/#obtaining-the-csb-dataset","title":"Obtaining the CSB dataset","text":"<p>Visit the following URL: https://www.nass.usda.gov/Research_and_Science/Crop-Sequence-Boundaries/index.php to download the 'Crop Sequence Boundaries 2016-2023' dataset. After downloading, extract the dataset, which includes crop-field shapes for the entire USA. You can then use crop_csb tool to clip it to fit your Area of Interest (AOI). If your study focuses on Maryland, clip the shapefile for Maryland and save it.</p>"},{"location":"csb/#clipping-the-csb-file-to-your-area-of-interest","title":"Clipping the CSB file to your Area of Interest","text":"<p>GeoEPIC comes with a command-line tool to filter and clip the CSB file for your specific region of interest. The tool can be invoked with various options to specify the region: <pre><code># Help with the options provided by the tool\n&gt;&gt; geo_epic crop_csb -h\n</code></pre> <pre><code># Cropping the csb to get the shapefile for Maryland:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --state_name \"Maryland\"\n</code></pre> <pre><code># Using the state FIPS code (24) for Maryland:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --state_fips \"24\"\n</code></pre> <pre><code># To get the shapefile for Montgomery County, Maryland:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --county_name \"Montgomery, Maryland\"\n</code></pre> <pre><code># Cropping the CSB using a bounding box:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --bbox \"-77.5,38.0,-76.0,39.5\"\n</code></pre> <pre><code># Cropping the csb around a center point with a specified extent in km:\n&gt;&gt; geo_epic crop_csb path/to/input.gdb output.shp --center \"39.0,-77.0\" --extent \"50x50\"\n</code></pre></p> <p>Note: For countries other than USA, a geo-tiff file covering the region of interest with one channel being the cultivated crop land mask of that region and another channel containing corresponding management info ID can be used to utilize GeoEPIC.</p>"},{"location":"epic/","title":"Environmental Policy Integrated Climate Model","text":"<p>The Environmental Policy Integrated Climate (EPIC) model, originally known as the Erosion/Productivity Impact Calculator, is designed to simulate a field, farm or small watershed, that is homogenous in terms of climate, soil, land use, and topography. EPIC is capable of simulating around eighty crops. This model helps in analyzing the effects of management strategies on soil erosion and extends to other aspects of agricultural sustainability like water quality, crop yields, plant competition, and pest control.</p> <p>For more details on EPIC model, visit Texas A&amp;M AgriLife site. </p> <p> </p> <p></p> <p>Initially developed in the early 1980s, (J. R. Williams, Jones, &amp; Dyke, 1984). EPIC integrated components from previous models such as CREAMS (Knisel, 1980) and SWRRB (J. R. Williams, Nicks, &amp; Arnold, 1985). The model was later enhanced with a pesticide component from GLEAMS (Leonard, 1987) to assess pesticide fate, further developed by Sabbagh, Geleta, Elliott, Williams, &amp; Griggs (1991). EPIC has evolved to address broader issues such as global climate change impacts, biomass production for energy, and landfill design, making it a versatile tool in environmental and agricultural management.</p> <p></p> <p>Refer to the user manual for detailed guidance on using EPIC: </p>"},{"location":"gee/","title":"Google Earth Engine Utility","text":"<p>Google Earth Engine (GEE) is a cloud-based platform for planetary-scale environmental data analysis. It hosts a vast collection of satellite imagery and other geospatial datasets, enabling users to perform large-scale data analysis. To explore the available datasets, visit Google Earth Engine's dataset catalog and GEE Community Catalog. Private assets can also be uploaded to Earth Engine, to use them in combination with existing datasets.</p> <p>This module can be used to combine various datasets and extract the required timeseries directly from Google Earth Engine.</p> <p></p>"},{"location":"gee/#1-configuration-file-breakdown","title":"1. Configuration file breakdown","text":"<p>This module utilizes a configuration file in YAML format to extract data from Google Earth Engine. The configuration file defines global parameters, Earth Engine collections, and derived variables. Below is a detailed explanation of each section.</p> <pre><code># filename: config.yml\n\n# Global parameters\nglobal_scope:\n  time_range: ['2016-01-01', '2021-12-31']\n  variables: ['nir', 'red', 'green', 'ndvi']  \n  resolution: 10\n\n# Specify Earth Engine collections and their respective variables\ncollections:\n\n  le07:\n    collection: LANDSAT/LE07/C02/T1_L2\n    select:  (b('QA_PIXEL') &gt;&gt; 6) &amp; 1\n    variables:\n      nir: b('SR_B4')*0.0000275 - 0.2\n      red: b('SR_B3')*0.0000275 - 0.2\n      green: b('SR_B2')*0.0000275 - 0.2\n\n# Derived variables\nderived_variables:\n  ndvi: '(nir - red)/(nir + red)'\n</code></pre>"},{"location":"gee/#a-global-parameters","title":"a) Global Parameters","text":"<p>The <code>global_scope</code> section contains general settings applicable to the entire data extraction process.</p> <ul> <li> <p><code>time_range</code>: Specifies the period for which the satellite data should be fetched.</p> </li> <li> <p><code>variables</code>: Lists the key variables to be extracted. These are typically satellite bands or derived products such as vegetation indices, or any other relevant parameters like tempurature etc.,</p> </li> <li> <p><code>resolution</code>: Defines the spatial resolution (in meters) for the output data.</p> </li> </ul>"},{"location":"gee/#b-earth-engine-collections","title":"b) Earth Engine Collections","text":"<p>Each Earth Engine dataset is defined under its own subsection within collections. The key components are collection, select, and variables. Multiple datasets can be specified, and if data from multiple collections are present for the same day, the module will return the mean of the variables across all collections.</p> <ul> <li> <p><code>collection</code>: Refers to the specific Google Earth Engine dataset identifier.</p> </li> <li> <p><code>select</code>: Defines a masking or selection condition for the data extraction.</p> </li> <li> <p><code>variables</code>: Specifies the bands or parameters to extract, along with any scaling factors or transformations.</p> </li> </ul>"},{"location":"gee/#c-derived-variables","title":"c) Derived Variables","text":"<p>Derived variables are calculated from the raw bands using mathematical expressions and functions available in numpy package.</p>"},{"location":"gee/#2-fetching-the-data","title":"2. Fetching the Data","text":"<p>You can use the following command-line interface to fetch time-series of required variables from Google Earth Engine into a CSV file:</p> <pre><code>geo_epic gee &lt;config-file&gt; --fetch &lt;roi&gt; --out &lt;output-path&gt;\n</code></pre> <p>The region of interest for fetching data can be provided in three formats:</p> <ul> <li>Latitude and Longitude: Use direct coordinates [latitude, longitude] <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch 40.5677 98.5505 --out ./out/sample.csv\n</code></pre></li> <li> <p>Shapefile (.shp): A shapefile containing the polygons of interest. It must contain a SiteID or FieldID column. The data will be stored in the name of FieldID or SiteID. Fetches one file for each polygon. <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch ./input/region.shp --out ./out\n</code></pre></p> </li> <li> <p>CSV File: A CSV file must contain a SiteID or FieldID column. Additionally, if using a CSV, it must include lat, lon columns. The data will be stored in the name of FieldID or SiteID. <pre><code>geo_epic gee ./landsat_ndvi.yml --fetch ./input/region.csv --out ./out\n</code></pre></p> </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Before starting the setup, ensure you have <code>wget</code> and <code>conda</code> installed.  Follow the links for corresponding installation guides.</p>"},{"location":"installation/#steps-to-set-up-the-geoepic-toolkit","title":"Steps to Set Up the GeoEPIC Toolkit","text":"<ol> <li>Create a virtual environment in conda <pre><code>conda create --name epic_env python=3.11.9\n</code></pre></li> <li> <p>Activate the environment <pre><code>conda activate epic_env\n</code></pre></p> </li> <li> <p>Install the GeoEPIC Toolkit </p> <p>Option 1: Install Directly from GitHub (recommended) <pre><code>pip install git+https://github.com/smarsGroup/geo-epic.git\n</code></pre> Option 2: Install locally (for developers) </p> <pre><code>This option is advisable only for developers.\n\n```bash\ngit clone https://github.com/smarsGroup/geo-epic.git\n```\n\n```bash\ncd geo-epic\n```\n\n```bash\npip install .\n```\n</code></pre> </li> <li> <p>**Contributing to GeoEPIC ** </p> <p>Creating Issues  If you find any bugs, have any suggestions or feature requests, please create an issue on GitHub.</p> <p>Pull Requests  If you want to contribute to the code base, please create a new branch and send a pull request. Also, contact us for any kind of collaboration.</p> </li> </ol> <p>Now, the GeoEPIC toolkit is sucessfully installed on the epic_env conda environment. All the commands and python API can be accessed via that conda environment. Happy coding!</p>"},{"location":"opt/","title":"Model Calibration","text":""},{"location":"opt/#overview","title":"Overview","text":"<p>The Calibration Module in GeoEPIC is developed to assist in tuning desired parameters involved in the EPIC model based on observational data, such as Leaf Area Index (LAI), Net Ecosystem Exchange (NEE), crop yield, or biomass. This allows model parameters to be refined to better reflect specific local conditions or experimental results.</p>"},{"location":"opt/#getting-started","title":"Getting Started","text":"<p>To begin calibration, import the calibration module from the package:</p>"},{"location":"opt/#todo","title":"&lt;ToDo&gt;","text":""},{"location":"runepic/","title":"Running an EPIC Experiment","text":""},{"location":"runepic/#1-create-new-workspace","title":"1. Create new workspace","text":"<p><pre><code>geo_epic workspace new -w Test\ncd Test\n</code></pre> It willl create a new workspace in the new directory named 'Test'. This 'Test' folder will automatically create sub-folders for EPIC model like model, opc, sites, soil, weather and a config.yml doc in it. You need to to go to Test folder before simulation starts.</p>"},{"location":"runepic/#2-edit-config-file-as-needed","title":"2. Edit config file as needed","text":"<p>This package is mainly designed for study regions in the USA. In the config.yml file, update the settings in the config file based on your area of interest (AOI) and preferences. For example, if you need to run the model for the state of Maryland, USA, modify these rows as follows:</p> <pre><code>EXPName: Nitrogen Assessment (Your Experiment Name) \nRegion: Maryland \ncode: MD \nFields_of_Interests: ./CropRotations/MDRotFilt.shp\n</code></pre> <p>Note: The MDRotFilt.shp file is the same one downloaded and placed in the CropRotations folder in the previous step. <pre><code>soil: \n   ssurgo_gdb: ./soil/gSSURGO_MD.gdb\n</code></pre> Guideline: The rule of thumb is to edit the configuration file using the region code specific to your study region, as done here using \"MD\" throughout the config file.</p>"},{"location":"runepic/#3-prepare-opc-file","title":"3. Prepare OPC File","text":"<p>OPC refers to the agricultural management practice files which is yet to automated. For now, you need to prpare management files and keep it in a new folder named 'OPC' inside the 'Test' directory.</p>"},{"location":"runepic/#4-prepare-the-workspace","title":"4. Prepare the workspace","text":"<p><pre><code>geo_epic workspace prepare\n</code></pre> This command will automatically pre-process the input files before simulation.</p>"},{"location":"runepic/#5-and-execute-the-simulations","title":"5. And execute the simulations","text":"<p><pre><code>geo_epic workspace run\n</code></pre> This command will simulate the operation/s and automatically save the results in a new folder named 'Output'. </p> <p>This command will also create a post_process.pynb doc which will have an example code to visualize the required parameters from ACY and DGN files. </p> <p>You can edit this code as per your requirements. You just have to identify the parameters and edit accordingly.</p>"},{"location":"runepic/#example-visualization","title":"Example Visualization","text":""},{"location":"runepic/#6-post-process-the-output-visualization","title":"6. Post-process the output visualization","text":"<p>You need to post-process the output files according to your interests. Generally, as an agricultural reserachers you need to process the DGN and ACY files.</p>"},{"location":"runepic/#for-post-processing","title":"For post-processing","text":"<p><pre><code>epic_pkg workspace post_process\n</code></pre> This will run the example code post_process.pynb which has been created in the Test folder. It will take a variable called 'YLDG' which denotes the yearly yield in t/ha/yr for all the sites and put it in a sepearte column corresponding to all the site ids with creatinh a yldg.csv file.</p>"},{"location":"runepic/#for-visualization","title":"For visualization","text":"<p><pre><code>epic_pkg workspace visualize\n</code></pre> It will simply plot the 'YLDG' variable corresponding to the site ids and crate a map for study region. </p>"},{"location":"runepic/#your-plot-will-look-like-this","title":"Your plot will look like this:","text":""},{"location":"runexp/","title":"Running the Model","text":""},{"location":"runexp/#todo","title":"&lt;ToDo&gt;","text":"<pre><code>from geoEpic import Site, EpicModel\n\n# initialise a site object\nsite = Site(opc = './continuous_corn.OPC',  # management file\n            dly = './1123455.DLY',          # daily weather file \n            sol = './Andisol.SOL',          # soil file\n            sit = './1.SIT')                # site file\n\n\n# initialise the EPIC model\nmodel = EpicModel(path = './model/EPIC2301dt20230820')\nmodel.setup(start_year = 2014, duration = 10)\nmodel.set_output_types(['ACY', 'DGN'])\n\n# run the simulation for the site\nmodel.run(site)\n\n# get the required outputs\nacy = model.outputs['ACY']\nmodel.close()\n</code></pre> <p>Loading from config files:</p> <pre><code>site = Site.from_config(lat = , lon = , config = './config.yml')\nmodel = EpicModel.from_config(config = './config.yml')\n\nmodel.run(site)\nmodel.close()\n</code></pre> <p>Example config file: <pre><code># Model details\nEPICModel: ./model/EPIC2301dt20230820\nstart_year: 1995\nduration: 25\noutput_types:\n  - ACY  # Annual Crop data file\n  - DGN  # Daily general output file\nlog_dir: ./log\noutput_dir: ./output\n</code></pre></p> <ul> <li>To edit the OPC, SOL or files in the epic model folder, you could use the epic_editor. the following command will copy the epiceditor in to your current folder.</li> </ul> <pre><code>&gt;&gt; GeoEPIC workspace add epic_editor\n</code></pre>"},{"location":"blog/","title":"Blog","text":""}]}